{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import argparse\n",
    "import mlflow\n",
    "from hyperopt import hp, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow, version 1.26.0\n"
     ]
    }
   ],
   "source": [
    "## Answer 1\n",
    "!mlflow --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Trip Data Green Records January, February, March\n",
    "db = pd.read_parquet(\"green01.parquet\")\n",
    "db2 = pd.read_parquet(\"green02.parquet\")\n",
    "db3 = pd.read_parquet(\"green03.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## January data\n",
    "db['PULocationID']=db['PULocationID'].replace(np.nan, -1)\n",
    "db['DOLocationID']=db['DOLocationID'].replace(np.nan, -1)\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "db[categorical] = db[categorical].astype(str)\n",
    "db [\"duration\"] = (db.lpep_dropoff_datetime - db.lpep_pickup_datetime) / pd.Timedelta('1 minute')\n",
    "db = db[db['duration'] <= 60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean january data and create some variables \n",
    "def preprocess(db: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    db['PU_DO'] = db['PULocationID'] + '_' + db['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "    dicts = db[categorical + numerical].to_dict(orient='records')\n",
    "    if fit_dv:\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    return X, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## February data\n",
    "db2['PULocationID']=db2['PULocationID'].replace(np.nan, -1)\n",
    "db2['DOLocationID']=db2['DOLocationID'].replace(np.nan, -1)\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "db2[categorical] = db2[categorical].astype(str)\n",
    "db2 [\"duration\"] = (db2.lpep_dropoff_datetime - db2.lpep_pickup_datetime) / pd.Timedelta('1 minute')\n",
    "db2 = db2[db2['duration'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean february\n",
    "#  data and create some variables \n",
    "def preprocess(db2: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    db2['PU_DO'] = db2['PULocationID'] + '_' + db2['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "    dicts = db2[categorical + numerical].to_dict(orient='records')\n",
    "    if fit_dv:\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    return X, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## March data\n",
    "db3['PULocationID']=db3['PULocationID'].replace(np.nan, -1)\n",
    "db3['DOLocationID']=db3['DOLocationID'].replace(np.nan, -1)\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "db3[categorical] = db3[categorical].astype(str)\n",
    "db3 [\"duration\"] = (db3.lpep_dropoff_datetime - db3.lpep_pickup_datetime) / pd.Timedelta('1 minute')\n",
    "db3 = db3[db3['duration'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean march data and create some variables \n",
    "def preprocess(db3: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    db3['PU_DO'] = db3['PULocationID'] + '_' + db3['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "    dicts = db3[categorical + numerical].to_dict(orient='records')\n",
    "    if fit_dv:\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    return X, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dependent variable (duration)\n",
    "target = 'duration'\n",
    "y_train = db[target].values\n",
    "y_valid = db2[target].values\n",
    "y_test = db3[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the data sets    \n",
    "dv = DictVectorizer()\n",
    "X_train, dv = preprocess(db, dv, fit_dv=True)\n",
    "X_valid, _ = preprocess(db2, dv, fit_dv=False)\n",
    "X_test, _ = preprocess(db3, dv, fit_dv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the space in the computer where are gping to be the data\n",
    "os.makedirs('/Users/agvg/Documents/DS/MLOPS/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define dump_pickle\n",
    "def dump_pickle(obj, filename):\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## Save dictvectorizer and datasets\n",
    "    # Answer 2 = 4 packages   \n",
    "dump_pickle(dv, os.path.join('/Users/agvg/Documents/DS/MLOPS/', \"dv.pkl\"))\n",
    "dump_pickle((X_train, y_train), os.path.join('/Users/agvg/Documents/DS/MLOPS/', \"train.pkl\"))\n",
    "dump_pickle((X_valid, y_valid), os.path.join('/Users/agvg/Documents/DS/MLOPS/', \"valid.pkl\"))\n",
    "dump_pickle((X_test, y_test), os.path.join('/Users/agvg/Documents/DS/MLOPS/', \"test.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load_Pickle \n",
    "def load_pickle(filename: str):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "valid = xgb.DMatrix(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/05/28 20:24:08 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2022/05/28 20:24:10 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    }
   ],
   "source": [
    "## Question 3 Train data_set\n",
    "# Answer 17 parameters\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "                mlflow.set_tag(\"model\",\"alexander\")\n",
    "                rf = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_pred = rf.predict(X_valid)\n",
    "                rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "                mlflow.log_metric(\"rmse\",rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 5 valid test\n",
    "## Answer 4 = \n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    " def objective(params):\n",
    "\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_valid)\n",
    "        rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    " search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 1, 20, 1)),\n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 10, 50, 1)),\n",
    "        'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 10, 1)),\n",
    "        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 4, 1)),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    " rstate = np.random.default_rng(42)  # for reproducible results\n",
    "fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=Trials(),\n",
    "        rstate=rstate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='./mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>,\n",
       " <Experiment: artifact_location='./mlruns/1', experiment_id='1', lifecycle_stage='active', name='nyc-taxi', tags={}>,\n",
       " <Experiment: artifact_location='./mlruns/4', experiment_id='4', lifecycle_stage='active', name='Random-forest-2', tags={}>,\n",
       " <Experiment: artifact_location='./mlruns/5', experiment_id='5', lifecycle_stage='active', name='random-forest-best-models', tags={}>]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question 6 Lauch the tracking server\n",
    "# Answer = 6,55 \n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_experiment(name='Test/NYC/Taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs= client.search_runs(\n",
    "experiment_ids='4',\n",
    "filter_string=\"\",\n",
    "run_view_type=ViewType.ACTIVE_ONLY,\n",
    "max_results=5,\n",
    "order_by=[\"metrics.rmse ASC\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'nyc_taxi_regressor' already exists. Creating a new version of this model...\n",
      "2022/05/29 15:07:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: nyc_taxi_regressor, version 2\n",
      "Created version '2' of model 'nyc_taxi_regressor'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1653858449024, current_stage='None', description=None, last_updated_timestamp=1653858449024, name='nyc_taxi_regressor', run_id='23b2baeb18d24287b03440611051c6ba', run_link=None, source='./mlruns/4/23b2baeb18d24287b03440611051c6ba/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=2>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = '23b2baeb18d24287b03440611051c6ba'\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "mlflow.register_model(model_uri=model_uri, name=\"nyc_taxi_regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs:/23b2baeb18d24287b03440611051c6ba/model'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: None, stage: None\n"
     ]
    }
   ],
   "source": [
    "model_name=\"nyc_taxi_regressor\"\n",
    "latest_versions = client.get_latest_versions(name=model_name)\n",
    "\n",
    "for version in latest_versions:\n",
    "    print(f\"version: {version.current_stage}, stage: {version.current_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1653856864690, current_stage='Staging', description=None, last_updated_timestamp=1653857417096, name='nyc_taxi_regressor', run_id='23b2baeb18d24287b03440611051c6ba', run_link=None, source='./mlruns/4/23b2baeb18d24287b03440611051c6ba/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = 1\n",
    "new_stage= \"Staging\"\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage,\n",
    "    archive_existing_versions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1653856864690, current_stage='Staging', description='The model version 1 was transitioned to Staging on 2022-05-29', last_updated_timestamp=1653857420584, name='nyc_taxi_regressor', run_id='23b2baeb18d24287b03440611051c6ba', run_link=None, source='./mlruns/4/23b2baeb18d24287b03440611051c6ba/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    description=f\"The model version {model_version} was transitioned to {new_stage} on {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/Users/agvg/Documents/DS/MLOPS'\n",
    "params = '/Users/agvg/Documents/DS/MLOPS/MLOPS/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-03.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor= '/Users/agvg/Documents/DS/MLOPS/MLOPS/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/agvg/Documents/DS/MLOPS/MLOPS/models'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.download_artifacts(run_id=run_id, path=preprocessor, dst_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 486 ms, sys: 94.5 ms, total: 580 ms\n",
      "Wall time: 753 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 6.672409854377293}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time test_model(name=model_name, stage=\"Staging\", X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1653856864690, current_stage='Production', description='The model version 1 was transitioned to Staging on 2022-05-29', last_updated_timestamp=1653858726065, name='nyc_taxi_regressor', run_id='23b2baeb18d24287b03440611051c6ba', run_link=None, source='./mlruns/4/23b2baeb18d24287b03440611051c6ba/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb87038cbf202022b98f4c1ae0c83d0502971b4d08fa9ee2dcec703af5217ce9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('exp-tracking-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
